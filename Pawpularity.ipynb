{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pawpularity.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WXnRbES619q"
      },
      "source": [
        "# Pawpularity Contest\n",
        "\n",
        "Submissions are scored on the root mean squared error **RMSE**.\n",
        "\n",
        "Guides to use:\n",
        "*   Good Paper ==> https://dl.acm.org/doi/pdf/10.1145/3209693.3209698\n",
        "*   Multi Input ==> https://www.kaggle.com/yaniv256/tensorflow-multi-input-pet-pawpularity-model\n",
        "*   Transfer Learning ==> https://tfhub.dev/\n",
        "\n",
        "Things to do in order to increase efficiency:\n",
        "1.  See correlation of Tags and Pawpularity and keep only the usefull ones!\n",
        "2.  Use Transfer Learning and get a better model like ResNet!\n",
        "3.  Add more tags to the dataset by using a pretrained model of classification\n",
        "4.  Try common techniques for dealing with imbalanced data like:\n",
        "  *  Class weighting\n",
        "  *  Oversampling\n",
        "5.  Try different Learning Rates and Optimizers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI1Ulp0uaNwE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler, TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.losses import MeanSquaredError, MeanSquaredLogarithmicError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APthzcCHOc3a"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiyG-KmGEapG"
      },
      "source": [
        "tensorflow.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQuDamCLvTiu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVN3OKFm0dFJ"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t8ERxLTauiu"
      },
      "source": [
        "df = pd.read_csv('./train.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KMgiwURvuTA"
      },
      "source": [
        "df['Id'] = df['Id'] + '.jpg';\n",
        "df['Id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyQVxa4lvzgf"
      },
      "source": [
        "fig = plt.figure(figsize = (15,10));\n",
        "ax = fig.gca();\n",
        "df['Pawpularity'].hist(ax = ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVAhjwMxwKQW"
      },
      "source": [
        "plt.figure(figsize=(15,10));\n",
        "ax = sns.heatmap(df.corr());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsb_SPs1wSEz"
      },
      "source": [
        "# Let's assign data a class 1-5 mapping 0-100 every 20 base on its Papwpularity \n",
        "df['Class'] = df['Pawpularity']/20;\n",
        "df['Class'] = df['Class'].apply(math.floor);\n",
        "df = df[['Id','Pawpularity','Class']]\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWBxz3YXwSgv"
      },
      "source": [
        "df_0 = df[df['Class']==0]\n",
        "df_1 = df[df['Class']==1]\n",
        "df_2 = df[df['Class']==2]\n",
        "df_3 = df[df['Class']==3]\n",
        "df_4 = df[df['Class']==4]\n",
        "# Don't forget those images with score 100\n",
        "df_5 = df[df['Class']==5]\n",
        "df_4 = pd.concat([df_4,df_5],axis=0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkHAALxMwSrz"
      },
      "source": [
        "len_0 = len(df_0);\n",
        "len_1 = len(df_1);\n",
        "len_2 = len(df_2);\n",
        "len_3 = len(df_3);\n",
        "len_4 = len(df_4);\n",
        "total = len_0 + len_1 + len_2 + len_3 + len_4;\n",
        "print(f'Total {total} rows');\n",
        "print(f'Pawpularity 0-20 is only {(len_0/total)*100:.2f}% percentage of data with {len_0} records');\n",
        "print(f'Pawpularity 20-40 is only {(len_1/total)*100:.2f}% percentage of data with {len_1} records');\n",
        "print(f'Pawpularity 40-60 is only {(len_2/total)*100:.2f}% percentage of data with {len_2} records');\n",
        "print(f'Pawpularity 60-80 is only {(len_3/total)*100:.2f}% percentage of data with {len_3} records');\n",
        "print(f'Pawpularity 80-100 is only {(len_4/total)*100:.2f}% percentage of data with {len_4} records');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2OvNUpRwS2Y"
      },
      "source": [
        "# If we keep only N from each class 1 and class 2 respectively the data would be more balanced\n",
        "keep_rows = 2500 #@param {type:\"slider\", min:1000, max:5000, step:100}\n",
        "df_1 = df_1.sample(keep_rows);\n",
        "if(keep_rows<len(df_2)):\n",
        "  df_2 = df_2.sample(keep_rows);\n",
        "balanced_df =  pd.concat([df_0,df_1,df_2,df_3,df_4],axis=0);\n",
        "balanced_df = balanced_df.sample(frac=1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Feu5831u1_w1"
      },
      "source": [
        "balanced_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsumwhwCw2c5"
      },
      "source": [
        "fig = plt.figure(figsize = (15,10));\n",
        "ax = fig.gca();\n",
        "balanced_df['Pawpularity'].hist(ax = ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJF7zfAcw2p8"
      },
      "source": [
        "# train_df, val_df = train_test_split(balanced_df,test_size=0.002);\n",
        "\n",
        "# print(f'Train Dataframe has {len(train_df)} records');\n",
        "# print(f'Validation Dataframe has {len(val_df)} records');\n",
        "keep_from_class = 3;\n",
        "values_0 = balanced_df[balanced_df['Class']==0].iloc[:keep_from_class];\n",
        "df_0 = balanced_df[balanced_df['Class']==0].iloc[keep_from_class:];\n",
        "values_1 = balanced_df[balanced_df['Class']==1].iloc[:keep_from_class];\n",
        "df_1 = balanced_df[balanced_df['Class']==1].iloc[keep_from_class:];\n",
        "values_2 = balanced_df[balanced_df['Class']==2].iloc[:keep_from_class];\n",
        "df_2 = balanced_df[balanced_df['Class']==2].iloc[keep_from_class:];\n",
        "values_3 = balanced_df[balanced_df['Class']==3].iloc[:keep_from_class];\n",
        "df_3 = balanced_df[balanced_df['Class']==3].iloc[keep_from_class:];\n",
        "values_4 = balanced_df[balanced_df['Class']==4].iloc[:keep_from_class];\n",
        "df_4 = balanced_df[balanced_df['Class']==4].iloc[keep_from_class:];\n",
        "value_5 = balanced_df[balanced_df['Class']==5].iloc[:keep_from_class];\n",
        "df_5 = balanced_df[balanced_df['Class']==5].iloc[keep_from_class:];\n",
        "train_df =  pd.concat([df_0,df_1,df_2,df_3,df_4, df_5],axis=0);\n",
        "train_df = train_df.sample(frac=1);\n",
        "val_df =  pd.concat([values_0,values_1,values_2,values_3,values_4, value_5],axis=0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKUC2iTm2aP7"
      },
      "source": [
        "val_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSE3zNbTYH1g"
      },
      "source": [
        "val_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VELhlqFAw21e"
      },
      "source": [
        "import random;\n",
        "rows, cols = 3, 3;\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(15,15));\n",
        "fig.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.4);\n",
        "for i in range(rows):\n",
        "    for j in range(cols):\n",
        "      random_image = random.randint(0,len(df)-1);\n",
        "      img = mpimg.imread('./train/'+df['Id'][random_image]);\n",
        "      axs[i,j].imshow(img);\n",
        "      axs[i,j].axis('off');\n",
        "      axs[i,j].set_title(f'Pawpularity: {df[\"Pawpularity\"][random_image]}',{'fontsize': 20});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhvg-v5RiwFj"
      },
      "source": [
        "InceptionResNetV2 = InceptionResNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_tensor=None,\n",
        "    input_shape=(300,300,3),\n",
        "    pooling='max'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx2GH9bsx07R"
      },
      "source": [
        "# train = tensorflow.data.Dataset.from_tensor_slices((train_images, rest_of_train_data)).map(preprocess).shuffle(216).batch(62).prefetch(2)\n",
        "# validation = tensorflow.data.Dataset.from_tensor_slices((val_images, rest_of_val_data)).map(preprocess).batch(10).prefetch(2)\n",
        "\n",
        "train_data_generator = ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2,\n",
        "    rescale = 1.0/255.0\n",
        ");\n",
        "\n",
        "val_data_generator = ImageDataGenerator(\n",
        "    rescale = 1.0/255.0\n",
        ");\n",
        "\n",
        "\n",
        "train = train_data_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=\"./train/\",\n",
        "    x_col=\"Id\",\n",
        "    y_col=\"Pawpularity\",\n",
        "    batch_size=24,\n",
        "    shuffle=True,\n",
        "    class_mode=\"raw\",\n",
        "    target_size=(300,300)\n",
        ");\n",
        "\n",
        "validation = val_data_generator.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    directory=\"./train/\",\n",
        "    x_col=\"Id\",\n",
        "    y_col=\"Pawpularity\",\n",
        "    batch_size=len(val_df),\n",
        "    shuffle=False,\n",
        "    class_mode=\"raw\",\n",
        "    target_size=(300,300)\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPpIqywgyBwR"
      },
      "source": [
        "#  Initialization\n",
        "epochs_count = 0;\n",
        "rmse_history = [];\n",
        "val_rmse_history = [];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkCrCgY3aNwO"
      },
      "source": [
        "image_input = Input(shape=(300, 300, 3));\n",
        "\n",
        "image_x = InceptionResNetV2(image_input);\n",
        "image_x = Flatten()(image_x);\n",
        "output = Dense(1, activation=\"linear\")(image_x);\n",
        "\n",
        "model = Model(inputs=image_input,outputs=output);\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEAxla0ZzemF"
      },
      "source": [
        "# Settings\n",
        "adam_lr =  0.0003#@param {type:\"slider\", min:0.0001, max:0.01, step:0.0001}\n",
        "sgd_lr =  0.01#@param {type:\"slider\", min:0.001, max:0.09, step:0.001}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo-Vi-GAzjKW"
      },
      "source": [
        "# tf.keras.optimizers.Adam(\n",
        "#     learning_rate=0.0003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "#     name='Adam', **kwargs\n",
        "# )\n",
        "\n",
        "# tf.keras.optimizers.SGD(\n",
        "#     learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\", **kwargs\n",
        "# )\n",
        "\n",
        "# Lets tweak learning rate to see rate of conversion\n",
        "adam = Adam(learning_rate = adam_lr);\n",
        "sgd = SGD(learning_rate = sgd_lr);\n",
        "mse_loss = MeanSquaredError();\n",
        "msle_loss  = MeanSquaredLogarithmicError (); #For SGD\n",
        "rmse = RootMeanSquaredError(name='rmse');\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='rmse', patience=3, verbose=1, factor=0.25, min_lr=0.00001);\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_rmse\",\n",
        "    min_delta=0.02,\n",
        "    patience=7,\n",
        "    verbose=1,\n",
        "    mode=\"min\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        ");\n",
        "\n",
        "def scheduler(epoch, learning_rate):\n",
        "    if epoch < 3:\n",
        "        new_learning_rate = learning_rate\n",
        "    else:  \n",
        "        new_learning_rate = learning_rate * tensorflow.math.exp(-0.1);\n",
        "    print(f'Learning rate = {new_learning_rate:.6f}');\n",
        "    return new_learning_rate;\n",
        "\n",
        "learning_scheduler = LearningRateScheduler(scheduler);\n",
        "\n",
        "model.compile(loss=mse_loss, optimizer=adam, metrics=['mae',rmse]);\n",
        "callbacks=[learning_scheduler, early_stop, reduce_lr];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaOXqGUKaNwq"
      },
      "source": [
        "epochs =  5#@param {type:\"slider\", min:1, max:300, step:10}\n",
        "history = model.fit(train, validation_data=validation, epochs=epochs, verbose=1, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM6v_K060JvD"
      },
      "source": [
        "epochs_count += epochs;\n",
        "rmse_history += history.history['rmse'];\n",
        "val_rmse_history += history.history['val_rmse'];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCu8-ran0TUy"
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "plt.plot(rmse_history[2:])\n",
        "plt.plot(val_rmse_history[2:])\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train_RMSE','Validation_RMSE'])\n",
        "plt.title(f'Adam LR:{adam_lr} + Epochs:{epochs_count} + Keep {keep_rows}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmBYs_zxuZR8"
      },
      "source": [
        "# model.save( 'my_model_' + datetime.datetime.today().strftime('%Y-%m-%d-%H:%M'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jADpYTse8T8R"
      },
      "source": [
        "# Create submissions csv\n",
        "\n",
        "For each Id in the test set, you must predict a probability for the target variable, Pawpularity. The file should contain a header and have the following format:\n",
        "\n",
        "Id, Pawpularity \\\n",
        "0008dbfb52aa1dc6ee51ee02adf13537, 99.24 \\\n",
        "0014a7b528f1682f0cf3b73a991c17a0, 61.71 \\\n",
        "0019c1388dfcd30ac8b112fb4250c251, 6.23 \\\n",
        "00307b779c82716b240a24f028b0031b, 9.43 \\\n",
        "00320c6dd5b4223c62a9670110d47911, 70.89 \\\n",
        "etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwzrp0bT8Uvm"
      },
      "source": [
        "test_df = pd.read_csv('./test.csv')\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrT6VeIU0BX5"
      },
      "source": [
        "test_df['file_path'] = test_df['Id'] + '.jpg';\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35n0GNsbzgFy"
      },
      "source": [
        "def test_preprocess(image, tabular):\n",
        "    image_string = tensorflow.io.read_file('./test/'+image);\n",
        "    image = tensorflow.image.decode_jpeg(image_string, channels=3);\n",
        "    image = tensorflow.cast(image, tensorflow.float32) / 255.0;\n",
        "    image = tensorflow.image.central_crop(image, 1.0);\n",
        "    image = tensorflow.image.resize(image, (300, 300));\n",
        "    return (image, tabular), 0\n",
        "\n",
        "test_images = test_df['file_path'];\n",
        "rest_of_test_data = test_df.drop('Id',axis=1);\n",
        "rest_of_test_data = rest_of_test_data.drop('file_path',axis=1);\n",
        "rest_of_test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zuNYbU00T4_"
      },
      "source": [
        "test = tensorflow.data.Dataset.from_tensor_slices((test_images, rest_of_test_data)).map(test_preprocess).batch(8).prefetch(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8mLXNZ50uxi"
      },
      "source": [
        "predicted_scores = model.predict(test).reshape(-1);\n",
        "predicted_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZNHuw1203bO"
      },
      "source": [
        "test_df['Pawpularity'] = predicted_scores;\n",
        "submission_df = test_df.reindex(['Id','Pawpularity'],axis=1);\n",
        "submission_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81rSiisI3THY"
      },
      "source": [
        "file_name = 'submission' + datetime.datetime.today().strftime('%Y-%m-%d-%H:%M') + '.csv';\n",
        "submission_df.to_csv(file_name, index=False);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}