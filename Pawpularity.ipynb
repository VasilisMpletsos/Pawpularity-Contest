{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pawpularity.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WXnRbES619q"
      },
      "source": [
        "# Pawpularity Contest\n",
        "\n",
        "Submissions are scored on the root mean squared error **RMSE**.\n",
        "\n",
        "Guides to use:\n",
        "*   Good Paper ==> https://dl.acm.org/doi/pdf/10.1145/3209693.3209698\n",
        "*   Multi Input ==> https://www.kaggle.com/yaniv256/tensorflow-multi-input-pet-pawpularity-model\n",
        "*   Transfer Learning ==> https://tfhub.dev/\n",
        "\n",
        "Things to do in order to increase efficiency:\n",
        "1.  See correlation of Tags and Pawpularity and keep only the usefull ones!\n",
        "2.  Use Transfer Learning and get a better model like ResNet!\n",
        "3.  Add more tags to the dataset by using a pretrained model of classification\n",
        "4.  Try common techniques for dealing with imbalanced data like:\n",
        "  *  Class weighting\n",
        "  *  Oversampling\n",
        "5.  Try different Learning Rates and Optimizers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI1Ulp0uaNwE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler, TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.losses import MeanSquaredError, MeanSquaredLogarithmicError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APthzcCHOc3a"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiyG-KmGEapG"
      },
      "source": [
        "tensorflow.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t8ERxLTauiu"
      },
      "source": [
        "tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkCrCgY3aNwO"
      },
      "source": [
        "image_input = Input(shape=(300, 300, 3))\n",
        "tabular_input = Input(len(tabular_columns))\n",
        "\n",
        "image_x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(image_input)\n",
        "image_x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(image_x)\n",
        "image_x = MaxPool2D((2,2))(image_x)\n",
        "image_x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(image_x)\n",
        "image_x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(image_x)\n",
        "image_x = MaxPool2D((2,2))(image_x)\n",
        "image_x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(image_x)\n",
        "image_x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(image_x)\n",
        "image_x = MaxPool2D((2,2))(image_x)\n",
        "image_x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(image_x)\n",
        "image_x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(image_x)\n",
        "image_x = MaxPool2D((2,2))(image_x)\n",
        "image_x = Flatten()(image_x)\n",
        "image_x = Dense(64, activation=\"relu\", kernel_regularizer=tensorflow.keras.regularizers.l2())(image_x)\n",
        "\n",
        "tabular_x = Dense(16, activation=\"relu\")(tabular_input)\n",
        "tabular_x = Dense(16, activation=\"relu\")(tabular_x)\n",
        "tabular_x = Dense(16, activation=\"relu\")(tabular_x)\n",
        "tabular_x = Dense(16, activation=\"relu\", kernel_regularizer=tensorflow.keras.regularizers.l2())(tabular_x)\n",
        "\n",
        "x = Concatenate(axis=1)([image_x, tabular_x])\n",
        "\n",
        "x = Dense(10, activation=\"relu\")(x)\n",
        "output = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "model = Model(inputs=[image_input, tabular_input],outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "887VhKZQdRMk"
      },
      "source": [
        "plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSXrIG2fGJJk"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVN3OKFm0dFJ"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwPAKC1AGikf"
      },
      "source": [
        "# !unzip petfinder-pawpularity-score.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEOY-L77aNwZ"
      },
      "source": [
        "df = pd.read_csv('./train.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8OQD6KtaNwi"
      },
      "source": [
        "df['Id'] = df['Id'] + '.jpg';\n",
        "df['Id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmovayz3aNwl"
      },
      "source": [
        "# For SGD we have to normalize data\n",
        "# df['Pawpularity'] = df['Pawpularity'] / df['Pawpularity'].max()\n",
        "df['Pawpularity']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50OuOmQFUEYK"
      },
      "source": [
        "fig = plt.figure(figsize = (15,10));\n",
        "ax = fig.gca();\n",
        "df['Pawpularity'].hist(ax = ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paBJXdlviRFo"
      },
      "source": [
        "## As we can see the data are imbalanced. We must do something about it later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yvp-pKBKQ0c"
      },
      "source": [
        "## Show 9 random images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPz29BmhUJxZ"
      },
      "source": [
        "import random;\n",
        "rows, cols = 3, 3;\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(15,15));\n",
        "fig.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.4);\n",
        "for i in range(rows):\n",
        "    for j in range(cols):\n",
        "      random_image = random.randint(0,len(df)-1);\n",
        "      img = mpimg.imread('./train/'+df['Id'][random_image]);\n",
        "      axs[i,j].imshow(img);\n",
        "      axs[i,j].axis('off')\n",
        "      axs[i,j].set_title(f'Pawpularity: {df[\"Pawpularity\"][random_image]}',{'fontsize': 20});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Rf7taCaNwn"
      },
      "source": [
        "def preprocess(image, tabular):\n",
        "    image_string = tensorflow.io.read_file('./train/'+image);\n",
        "    image = tensorflow.image.decode_jpeg(image_string, channels=3);\n",
        "    image = tensorflow.cast(image, tensorflow.float32) / 255.0;\n",
        "    image = tensorflow.image.central_crop(image, 1.0);\n",
        "    image = tensorflow.image.resize(image, (300, 300));\n",
        "    return (image, tabular[0:12]), tabular[12]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N85RzL7gW9q4"
      },
      "source": [
        "images = df['Id'];\n",
        "rest_of_data = df.drop('Id',axis=1);\n",
        "rest_of_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZYxLkTGrgUj"
      },
      "source": [
        "# TO DO\n",
        "\n",
        "* Find correlation and keep only tags that matter\n",
        "* Guide https://towardsdatascience.com/annotated-heatmaps-in-5-simple-steps-cc2a0660a27d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PeVeWQIW4cc"
      },
      "source": [
        "train = tensorflow.data.Dataset.from_tensor_slices((images, rest_of_data)).map(preprocess).shuffle(216).batch(62).prefetch(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsj6z7zS6Z8h"
      },
      "source": [
        "Despite the widespread popularity of Adam, recent research papers have noted that it can fail to converge to an optimal solution under specific settings. The paper Improving Generalization Performance by Switching from Adam to SGD demonstrates that adaptive optimization techniques such as Adam generalize poorly compared to SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wn7oauu7C28"
      },
      "source": [
        "# Settings\n",
        "adam_lr =  0.004#@param {type:\"slider\", min:0.0001, max:0.01, step:0.0001}\n",
        "sgd_lr =  0.01#@param {type:\"slider\", min:0.001, max:0.09, step:0.001}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLLrvXr-SciL"
      },
      "source": [
        "# tf.keras.optimizers.Adam(\n",
        "#     learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "#     name='Adam', **kwargs\n",
        "# )\n",
        "\n",
        "# tf.keras.optimizers.SGD(\n",
        "#     learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\", **kwargs\n",
        "# )\n",
        "\n",
        "# Lets tweak learning rate to see rate of conversion\n",
        "adam = Adam(learning_rate = adam_lr);\n",
        "sgd = SGD(learning_rate = sgd_lr);\n",
        "mse_loss = MeanSquaredError();\n",
        "msle_loss  = MeanSquaredLogarithmicError (); #For SGD\n",
        "rmse = RootMeanSquaredError(name='rmse');\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.today().strftime('%Y-%m-%d-%H:%M'))\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='rmse', patience=3, verbose=1, factor=0.75, min_lr=0.00001);\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"rmse\",\n",
        "    min_delta=0.01,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode=\"min\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        ");\n",
        "\n",
        "def scheduler(epoch, learning_rate):\n",
        "  if epoch < 5:\n",
        "    return learning_rate\n",
        "  elif epoch >= 5:\n",
        "    # tensorflow.math.exp(-0.1) equals ~0.9\n",
        "    return learning_rate * tensorflow.math.exp(-0.1)\n",
        "\n",
        "learning_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "model.compile(loss=mse_loss, optimizer=adam, metrics=['mae',rmse]);\n",
        "callbacks=[tensorboard_callback, reduce_lr, early_stop];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaOXqGUKaNwq"
      },
      "source": [
        "epochs =  100#@param {type:\"slider\", min:10, max:300, step:10}\n",
        "model.fit(train, epochs=epochs, verbose=1, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfX9_rCYaNws"
      },
      "source": [
        "path = logdir + '/train'\n",
        "%tensorboard --logdir path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmBYs_zxuZR8"
      },
      "source": [
        "model.save( 'my_model_' + datetime.datetime.today().strftime('%Y-%m-%d-%H:%M'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jADpYTse8T8R"
      },
      "source": [
        "# Create submissions csv\n",
        "\n",
        "For each Id in the test set, you must predict a probability for the target variable, Pawpularity. The file should contain a header and have the following format:\n",
        "\n",
        "Id, Pawpularity \\\n",
        "0008dbfb52aa1dc6ee51ee02adf13537, 99.24 \\\n",
        "0014a7b528f1682f0cf3b73a991c17a0, 61.71 \\\n",
        "0019c1388dfcd30ac8b112fb4250c251, 6.23 \\\n",
        "00307b779c82716b240a24f028b0031b, 9.43 \\\n",
        "00320c6dd5b4223c62a9670110d47911, 70.89 \\\n",
        "etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwzrp0bT8Uvm"
      },
      "source": [
        "test_df = pd.read_csv('./test.csv')\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrT6VeIU0BX5"
      },
      "source": [
        "test_df['file_path'] = test_df['Id'] + '.jpg';\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35n0GNsbzgFy"
      },
      "source": [
        "def test_preprocess(image, tabular):\n",
        "    image_string = tensorflow.io.read_file('./test/'+image);\n",
        "    image = tensorflow.image.decode_jpeg(image_string, channels=3);\n",
        "    image = tensorflow.cast(image, tensorflow.float32) / 255.0;\n",
        "    image = tensorflow.image.central_crop(image, 1.0);\n",
        "    image = tensorflow.image.resize(image, (300, 300));\n",
        "    return (image, tabular), 0\n",
        "\n",
        "test_images = test_df['file_path'];\n",
        "rest_of_test_data = test_df.drop('Id',axis=1);\n",
        "rest_of_test_data = rest_of_test_data.drop('file_path',axis=1);\n",
        "rest_of_test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zuNYbU00T4_"
      },
      "source": [
        "test = tensorflow.data.Dataset.from_tensor_slices((test_images, rest_of_test_data)).map(test_preprocess).batch(8).prefetch(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8mLXNZ50uxi"
      },
      "source": [
        "predicted_scores = model.predict(test).reshape(-1);\n",
        "predicted_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZNHuw1203bO"
      },
      "source": [
        "test_df['Pawpularity'] = predicted_scores;\n",
        "submission_df = test_df.reindex(['Id','Pawpularity'],axis=1);\n",
        "submission_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81rSiisI3THY"
      },
      "source": [
        "file_name = 'submission' + datetime.datetime.today().strftime('%Y-%m-%d-%H:%M') + '.csv';\n",
        "submission_df.to_csv(file_name, index=False);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}